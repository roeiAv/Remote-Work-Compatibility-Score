{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmhi2mLN2gN3",
    "outputId": "c9aa51df-f46a-4f44-932d-b297d923c599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.19.0-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.19.0 trio-0.25.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wL4YxDCs4nQz",
    "outputId": "d1ad20a9-1880-4bb9-e824-755ea268a96f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [783 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,356 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,048 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,906 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,091 kB]\n",
      "Hit:12 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
      "Hit:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,080 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,624 kB]\n",
      "Fetched 11.1 MB in 4s (2,802 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
      "  udev\n",
      "Suggested packages:\n",
      "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
      "The following NEW packages will be installed:\n",
      "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
      "  systemd-hwe-hwdb udev\n",
      "The following packages will be upgraded:\n",
      "  libudev1\n",
      "1 upgraded, 9 newly installed, 0 to remove and 43 not upgraded.\n",
      "Need to get 27.3 MB of archives.\n",
      "After this operation, 114 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.3 [595 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.61.3+22.04 [24.7 MB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
      "Fetched 27.3 MB in 2s (14.7 MB/s)\n",
      "Preconfiguring packages ...\n",
      "Selecting previously unselected package apparmor.\n",
      "(Reading database ... 121753 files and directories currently installed.)\n",
      "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.3_amd64.deb ...\n",
      "Unpacking apparmor (3.0.4-2ubuntu2.3) ...\n",
      "Selecting previously unselected package liblzo2-2:amd64.\n",
      "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
      "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
      "Selecting previously unselected package squashfs-tools.\n",
      "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
      "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
      "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
      "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
      "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
      "Selecting previously unselected package udev.\n",
      "(Reading database ... 121961 files and directories currently installed.)\n",
      "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
      "Unpacking udev (249.11-0ubuntu3.12) ...\n",
      "Selecting previously unselected package libfuse3-3:amd64.\n",
      "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
      "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
      "Selecting previously unselected package snapd.\n",
      "Preparing to unpack .../snapd_2.61.3+22.04_amd64.deb ...\n",
      "Unpacking snapd (2.61.3+22.04) ...\n",
      "Setting up apparmor (3.0.4-2ubuntu2.3) ...\n",
      "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
      "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
      "Setting up squashfs-tools (1:4.5-3build1) ...\n",
      "Setting up udev (249.11-0ubuntu3.12) ...\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
      "Setting up snapd (2.61.3+22.04) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
      "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
      "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
      "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
      "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
      "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
      "Selecting previously unselected package chromium-browser.\n",
      "(Reading database ... 122191 files and directories currently installed.)\n",
      "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
      "=> Installing the chromium snap\n",
      "==> Checking connectivity with the snap store\n",
      "===> System doesn't have a working snapd, skipping\n",
      "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
      "Selecting previously unselected package chromium-chromedriver.\n",
      "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
      "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
      "Selecting previously unselected package systemd-hwe-hwdb.\n",
      "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
      "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
      "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
      "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
      "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
      "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
      "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
      "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
      "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update # to update ubuntu to correctly run apt install\n",
    "!apt install -yq chromium-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ItubNqw743KV"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rkbf2mHo49KY"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NCzCWIBL4tZv"
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.chromium.remote_connection import ChromiumRemoteConnection\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup,Comment\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping 1000 job postings!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "AUTH = 'brd-customer-hl_80709a30-zone-scraping_browser_newtsuf:w1ziplb2u1xv'\n",
    "SBR_WEBDRIVER = f'https://{AUTH}@brd.superproxy.io:9515'\n",
    "\n",
    "def append_to_json_file(file_path, new_data):\n",
    "    try:\n",
    "        with open(file_path, 'r+') as file:\n",
    "            # Read existing data\n",
    "            file_data = json.load(file)\n",
    "            # Append new data\n",
    "            file_data.extend(new_data)\n",
    "            # Move the pointer to the beginning of the file\n",
    "            file.seek(0)\n",
    "            # Write the updated data\n",
    "            json.dump(file_data, file, indent=4)\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, create it with the new data\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(new_data, file, indent=4)\n",
    "\n",
    "\n",
    "def get_html_as_json(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Adjust depending on page load times\n",
    "    page_source = driver.page_source\n",
    "    return {\"url\": url, \"html\": page_source}\n",
    "\n",
    "\n",
    "def get_details_from_drive(url, page_source):\n",
    "    print(url)\n",
    "    # driver.get(url)\n",
    "    # time.sleep(1)\n",
    "    # page_source = driver.page_source\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # with the help of beautifulSoup and html parser create soup\n",
    "\n",
    "    # Scrape the required details\n",
    "    dict_job = {}\n",
    "    # Find the job title\n",
    "    job_title_tag = soup.find('h1', {'class': 'top-card-layout__title'})\n",
    "    dict_job['job_title'] = job_title_tag.get_text(strip=True) if job_title_tag else 'Not available'\n",
    "    company_name_tag=soup.find('a', {'class': 'topcard__org-name-link'})\n",
    "    dict_job['company_name'] = company_name_tag.get_text(strip=True) if company_name_tag  else 'Not available'\n",
    "    company_location_tag=soup.find('span', {'class': 'topcard__flavor--bullet'})\n",
    "    dict_job['location'] = company_location_tag.get_text(strip=True) if company_location_tag else 'Not available'\n",
    "    # print(dict_job)\n",
    "\n",
    "    about_tag=soup.find('div', {'class': 'show-more-less-html__markup'})\n",
    "    dict_job['about'] = about_tag.get_text(strip=True) if about_tag else 'Not available'\n",
    "\n",
    "    # Scrape the workplace type, employment type, and level\n",
    "    # Find all li elements with the class 'description__job-criteria-item'\n",
    "    job_criteria_items = soup.find_all('li', {'class':'description__job-criteria-item'})\n",
    "    # Iterate through each li element and extract the required info\n",
    "    for item in job_criteria_items:\n",
    "        subheader = item.find('h3', class_='description__job-criteria-subheader')\n",
    "        if subheader and subheader.get_text(strip=True).lower() == 'seniority level':\n",
    "            seniority_level_tag=item.find('span', class_='description__job-criteria-text description__job-criteria-text--criteria')\n",
    "            dict_job['seniority_level'] = seniority_level_tag.get_text(strip=True) if seniority_level_tag else 'Not available'\n",
    "        elif subheader and subheader.get_text(strip=True).lower() == 'employment type':\n",
    "            empolyment_type_tag=item.find('span', class_='description__job-criteria-text description__job-criteria-text--criteria')\n",
    "            dict_job['employment_type'] = empolyment_type_tag.get_text(strip=True) if empolyment_type_tag else 'Not available'\n",
    "\n",
    "\n",
    "\n",
    "    # # Find the script tag containing the JSON data\n",
    "    # json_data = None\n",
    "    # script_tags = soup.find_all('script', {'type': 'application/ld+json'})\n",
    "    # for script in script_tags:\n",
    "    #     try:\n",
    "    #         json_data = json.loads(script.string)\n",
    "    #         if '@type' in json_data and json_data['@type'] == 'JobPosting':\n",
    "    #             break\n",
    "    #     except json.JSONDecodeError:\n",
    "    #         continue  # Skip if not a valid JSON\n",
    "    return {\"url\": url, \"dict\": dict_job}\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('Connecting to Scraping Browser...')\n",
    "    sbr_connection = ChromiumRemoteConnection(SBR_WEBDRIVER, 'goog', 'chrome')\n",
    "    with Remote(sbr_connection, options=ChromeOptions()) as driver:\n",
    "        current_job_count = 0\n",
    "        page_number = 45\n",
    "\n",
    "        while current_job_count < 1000:\n",
    "            driver.get(f'https://www.linkedin.com/jobs/search/?origin=JOB_SEARCH_PAGE_JOB_FILTER?start={page_number * 25}')\n",
    "            try:\n",
    "        # Wait for the job listings to be present\n",
    "                job_listings = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.base-card__full-link\")))\n",
    "                # Extract hrefs\n",
    "                hrefs = [listing.get_attribute('href') for listing in job_listings]\n",
    "            except Exception as e:\n",
    "                print(\"An error occurred:\", e)\n",
    "                break  # Break the loop if an error occurs\n",
    "            batch_content = []\n",
    "            for href in hrefs:\n",
    "                html_data = get_html_as_json(driver, href)\n",
    "                job_details = get_details_from_drive(html_data['url'], html_data['html'])\n",
    "                batch_content.append(job_details)\n",
    "                current_job_count += 1\n",
    "                if current_job_count >= 1000:\n",
    "                    break\n",
    "\n",
    "            # Append batch content to JSON file\n",
    "            append_to_json_file('scraped_data_job_postings_20_03.json', batch_content)\n",
    "\n",
    "            # Increment page number to move to the next page\n",
    "            page_number += 1\n",
    "            #print(f'Processed page {page_number}. Total jobs processed: {current_job_count}')\n",
    "\n",
    "    print('Finished scraping 1000 job postings!')\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
